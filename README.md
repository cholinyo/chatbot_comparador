# ğŸ§  Prototipo de Chatbot Interno para Administraciones Locales

> **Trabajo Final de MÃ¡ster - Sistemas Inteligentes**  
> ComparaciÃ³n entre Modelos de Lenguaje Locales vs OpenAI usando RAG

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![Flask](https://img.shields.io/badge/Flask-2.3+-green.svg)](https://flask.palletsprojects.com/)
[![LangChain](https://img.shields.io/badge/LangChain-0.1+-purple.svg)](https://langchain.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un **chatbot conversacional interno** diseÃ±ado especÃ­ficamente para administraciones locales, que permite al personal tÃ©cnico municipal consultar informaciÃ³n institucional mediante lenguaje natural. El sistema compara el rendimiento entre **modelos de lenguaje locales** (usando Ollama) y **servicios comerciales** (OpenAI) mediante una arquitectura **RAG (Retrieval-Augmented Generation)**.

### ğŸ¯ Objetivos del TFM

- **Objetivo Principal**: Evaluar la viabilidad de modelos locales vs servicios comerciales en entornos de administraciÃ³n pÃºblica
- **SoberanÃ­a TecnolÃ³gica**: Implementar soluciones que cumplan con el CCN-TEC 014 y el Esquema Nacional de Seguridad
- **Arquitectura RAG**: Integrar mÃºltiples fuentes de conocimiento (documentos, APIs, bases de datos, web)
- **ComparaciÃ³n EmpÃ­rica**: Analizar mÃ©tricas de rendimiento, latencia, costes y calidad de respuestas

---

## ğŸ—ï¸ Arquitectura del Sistema

```mermaid
graph TB
    A[Usuario] --> B[Flask Web App]
    B --> C[Comparador de Modelos]
    C --> D[LangChain OpenAI]
    C --> E[LangChain Ollama]
    
    F[Sistema RAG] --> G[Vector Store FAISS]
    F --> H[Ingesta Multi-fuente]
    
    H --> I[ğŸ“„ Documentos]
    H --> J[ğŸŒ URLs]
    H --> K[ğŸ”Œ APIs]
    H --> L[ğŸ—ƒï¸ Bases de Datos]
    
    G --> C
    
    subgraph "Modelos Locales"
        E --> M[Llama 3.1]
        E --> N[Mistral 7B]
        E --> O[Gemma]
    end
    
    subgraph "Servicios Comerciales"
        D --> P[GPT-4]
        D --> Q[GPT-3.5]
    end
```

### ğŸ”§ Componentes Clave

1. **Frontend Web**: Interfaz Flask con Bootstrap para comparaciÃ³n de modelos
2. **Backend RAG**: Sistema de recuperaciÃ³n aumentada con LangChain
3. **Vector Store**: FAISS para bÃºsqueda semÃ¡ntica eficiente
4. **Ingesta Multi-fuente**: Pipelines para documentos, APIs, web y BBDD
5. **Comparador de Modelos**: EvaluaciÃ³n lado a lado con mÃ©tricas detalladas

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- **Python 3.9+**
- **Git**
- **Ollama** (para modelos locales)
- **OpenAI API Key** (opcional, para comparaciÃ³n)

### 1. Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/chatbot_comparador.git
cd chatbot_comparador
```

### 2. Crear Entorno Virtual

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# Windows:
venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Variables de Entorno

Crear archivo `.env` en la raÃ­z del proyecto:

```env
# OpenAI (opcional)
OPENAI_API_KEY=sk-your-key-here

# ConfiguraciÃ³n Flask
FLASK_ENV=development
FLASK_DEBUG=True

# Base de datos (futuro)
DATABASE_URL=sqlite:///data.db
```

### 5. Instalar y Configurar Ollama

```bash
# Instalar Ollama (Windows)
winget install Ollama.Ollama

# Iniciar servidor Ollama
ollama serve

# Descargar modelos (en otra terminal)
ollama pull llama3.1
ollama pull mistral
ollama pull gemma:7b
```

### 6. Inicializar Vector Store

```bash
# Crear directorios necesarios
mkdir -p vectorstore/{documents,web,apis,bbdd}
mkdir -p logs

# Ejecutar ingesta inicial (opcional)
python -m app.services.ingest_documents
```

---

## ğŸ’¡ Uso del Sistema

### Iniciar la AplicaciÃ³n

```bash
# Desarrollo
python run.py

# O usando Flask directamente
flask run --host=0.0.0.0 --port=5000
```

La aplicaciÃ³n estarÃ¡ disponible en: `http://localhost:5000`

### NavegaciÃ³n Principal

- **`/chat`** - Chat unificado con RAG
- **`/admin`** - AdministraciÃ³n de modelos
- **`/admin/model-tuning`** - ConfiguraciÃ³n avanzada de parÃ¡metros
- **`/config`** - ConfiguraciÃ³n de fuentes RAG
- **`/vectorstore`** - Estado del vector store
- **`/comparar`** - ComparaciÃ³n lado a lado de modelos

### Configurar Fuentes de Conocimiento

1. **Documentos**: AÃ±adir carpetas con PDFs, DOCXs, TXTs en `/config`
2. **URLs**: Configurar crawling de sitios web institucionales
3. **APIs**: Integrar APIs municipales o gubernamentales
4. **Bases de Datos**: Conectar con sistemas SQL existentes

---

## ğŸ“Š Estructura del Proyecto

```
chatbot_comparador/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py              # Factory Flask
â”‚   â”œâ”€â”€ routes/                  # Blueprints
â”‚   â”‚   â”œâ”€â”€ chat.py             # Chat unificado
â”‚   â”‚   â”œâ”€â”€ admin.py            # AdministraciÃ³n
â”‚   â”‚   â”œâ”€â”€ config.py           # ConfiguraciÃ³n RAG
â”‚   â”‚   â”œâ”€â”€ comparador.py       # ComparaciÃ³n modelos
â”‚   â”‚   â””â”€â”€ vectorstore.py      # GestiÃ³n vectorstore
â”‚   â”œâ”€â”€ services/               # LÃ³gica de negocio
â”‚   â”‚   â”œâ”€â”€ bot_openai.py       # Servicio OpenAI + LangChain
â”‚   â”‚   â”œâ”€â”€ bot_local.py        # Servicio Ollama + LangChain
â”‚   â”‚   â”œâ”€â”€ ingest_documents.py # Ingesta documentos
â”‚   â”‚   â”œâ”€â”€ ingest_web.py       # Ingesta web
â”‚   â”‚   â””â”€â”€ ingest_api.py       # Ingesta APIs
â”‚   â”œâ”€â”€ utils/                  # Utilidades
â”‚   â”‚   â”œâ”€â”€ rag_utils.py        # Funciones RAG
â”‚   â”‚   â””â”€â”€ doc_loader.py       # Carga documentos
â”‚   â”œâ”€â”€ templates/              # Plantillas HTML
â”‚   â”‚   â”œâ”€â”€ base.html
â”‚   â”‚   â”œâ”€â”€ chat.html
â”‚   â”‚   â”œâ”€â”€ admin.html
â”‚   â”‚   â”œâ”€â”€ model_tuning.html
â”‚   â”‚   â””â”€â”€ comparar.html
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ settings.json       # ConfiguraciÃ³n sistema
â”œâ”€â”€ vectorstore/                # Ãndices FAISS
â”‚   â”œâ”€â”€ documents/
â”‚   â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ apis/
â”‚   â””â”€â”€ bbdd/
â”œâ”€â”€ logs/                       # Logs del sistema
â”œâ”€â”€ scripts/                    # Scripts utilitarios
â”œâ”€â”€ requirements.txt            # Dependencias Python
â”œâ”€â”€ run.py                     # Lanzador principal
â””â”€â”€ README.md                  # Este archivo
```

---

## ğŸ§ª EvaluaciÃ³n y MÃ©tricas

### MÃ©tricas Implementadas

1. **Rendimiento**:
   - Latencia de respuesta (segundos)
   - Tokens procesados por segundo
   - Uso de memoria y CPU

2. **Calidad**:
   - Relevancia de fragmentos recuperados
   - Coherencia de respuestas generadas
   - Trazabilidad de fuentes

3. **Costes**:
   - Coste por consulta (OpenAI)
   - Recursos computacionales (modelos locales)
   - Escalabilidad econÃ³mica

### Casos de Uso Evaluados

- **Consultas normativas**: "Â¿QuÃ© ordenanza regula los ruidos en el municipio?"
- **Procedimientos administrativos**: "Â¿CÃ³mo solicitar una licencia de obra?"
- **InformaciÃ³n municipal**: "Â¿DÃ³nde estÃ¡n las cÃ¡maras de trÃ¡fico?"
- **TramitaciÃ³n electrÃ³nica**: "Â¿CÃ³mo consulto el estado de mi expediente?"

---

## ğŸ”§ Desarrollo y ContribuciÃ³n

### Estructura de Ramas

- `main` - VersiÃ³n estable
- `develop` - Desarrollo activo
- `feature/*` - Nuevas funcionalidades
- `fix/*` - Correcciones de errores

### Scripts Ãštiles

```bash
# Reindexar fuentes RAG
python -m app.services.ingest_documents
python -m app.services.ingest_web
python -m app.services.ingest_api

# Tests (futuro)
python -m pytest tests/

# Formateo de cÃ³digo
black app/
flake8 app/
```

### AÃ±adir Nuevos Modelos

1. **Modelo Local**: Agregar en `bot_local.py` y descargar con `ollama pull`
2. **Modelo Comercial**: Actualizar `bot_openai.py` con nueva configuraciÃ³n
3. **ConfiguraciÃ³n**: AÃ±adir parÃ¡metros en `/admin/model-tuning`

---

## ğŸ“ˆ Resultados del TFM

### Hallazgos Principales

1. **Latencia**: Los modelos locales tienen latencia variable (1-10s) vs OpenAI consistente (1-3s)
2. **Calidad**: GPT-4 superior en comprensiÃ³n compleja, modelos locales competitivos en tareas especÃ­ficas
3. **Costes**: Modelos locales coste fijo inicial, OpenAI coste variable por uso
4. **Privacidad**: Modelos locales cumplen mejor con requisitos de soberanÃ­a de datos

### Recomendaciones para Administraciones

- **Uso hÃ­brido**: Modelos locales para consultas frecuentes, OpenAI para casos complejos
- **ImplementaciÃ³n gradual**: Comenzar con modelos locales pequeÃ±os, escalar segÃºn necesidades
- **Governance**: Establecer polÃ­ticas claras de uso y supervisiÃ³n humana

---

## ğŸ›¡ï¸ Seguridad y Cumplimiento

### Cumplimiento Normativo

- **ENS (Esquema Nacional de Seguridad)**: SeparaciÃ³n de mÃ³dulos, logs de auditorÃ­a
- **CCN-TEC 014**: Recomendaciones para sistemas con IA
- **RGPD**: ProtecciÃ³n de datos personales, anonimizaciÃ³n
- **Transparencia**: Trazabilidad de respuestas y fuentes

### Medidas de Seguridad

- AutenticaciÃ³n de usuarios (futuro)
- Cifrado de comunicaciones
- Logs de auditorÃ­a completos
- ValidaciÃ³n de entradas
- Rate limiting API

---

## ğŸš€ Despliegue en ProducciÃ³n

### Opciones de Despliegue

1. **Docker** (recomendado):
```bash
# Crear imagen
docker build -t chatbot-comparador .

# Ejecutar contenedor
docker run -p 5000:5000 chatbot-comparador
```

2. **Cloud Platforms**:
   - **Azure**: App Service + Container Instance
   - **AWS**: EC2 + ECS
   - **Google Cloud**: Cloud Run + Compute Engine

3. **On-Premise**:
   - Servidor Ubuntu/CentOS
   - Nginx como proxy reverso
   - Systemd para gestiÃ³n de servicios

### Variables de Entorno ProducciÃ³n

```env
FLASK_ENV=production
FLASK_DEBUG=False
OPENAI_API_KEY=sk-prod-key
DATABASE_URL=postgresql://user:pass@host:5432/db
OLLAMA_HOST=http://ollama-server:11434
```

---

## ğŸ“š DocumentaciÃ³n Adicional

### ArtÃ­culos Relacionados

- [GuÃ­a CCN-TEC 014 para IA en AAPP](https://www.ccn-cert.cni.es/)
- [Estrategia Nacional de IA 2024](https://portal.mineco.gob.es/)
- [LangChain Documentation](https://python.langchain.com/)
- [Ollama Model Library](https://ollama.ai/library)

### Papers de Referencia

1. *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks* (Lewis et al., 2020)
2. *LLaMA: Open and Efficient Foundation Language Models* (Touvron et al., 2023)
3. *GPT-4 Technical Report* (OpenAI, 2023)

---

## ğŸ› Problemas Conocidos

### Issues Comunes

1. **Ollama no responde**: Verificar que `ollama serve` estÃ© ejecutÃ¡ndose
2. **OpenAI rate limit**: Implementar rate limiting en cÃ³digo
3. **Memoria insuficiente**: Modelos locales requieren 8GB+ RAM
4. **Lentitud en ingesta**: Procesar documentos grandes en lotes

### Soluciones

```bash
# Verificar estado Ollama
curl http://localhost:11434/api/tags

# Limpiar vector store
rm -rf vectorstore/*/
python -m app.services.ingest_documents

# Verificar logs
tail -f logs/admin.log
```

---

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/AmazingFeature`)
3. Commit cambios (`git commit -m 'Add AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir Pull Request

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

---

## ğŸ‘¨â€ğŸ’» Autor

**Vicente Caruncho Ramos**  
ğŸ“§ Email: -
ğŸ“ MÃ¡ster en Sistemas Inteligentes - Universitat Jaume I  
ğŸ“… Curso: 2024-2025

---

## ğŸ™ Agradecimientos

- **Tutor**: -
- **LangChain Community** por el framework
- **Ollama Team** por simplificar modelos locales
- **OpenAI** por proporcionar API de comparaciÃ³n
- **Comunidad Flask** por la base web

---

## ğŸ“Š Estado del Proyecto

- âœ… **Fase 1**: Arquitectura base y comparador bÃ¡sico
- âœ… **Fase 2**: IntegraciÃ³n LangChain y RAG
- ğŸ”„ **Fase 3**: EvaluaciÃ³n empÃ­rica y mÃ©tricas (en progreso)
- â³ **Fase 4**: DocumentaciÃ³n final TFM
- â³ **Fase 5**: Despliegue en cloud para demostraciones

---

## ğŸ“ Soporte

Si encuentras problemas o tienes preguntas:

1. **Issues**: Abre un issue en GitHub
2. **DocumentaciÃ³n**: Consulta este README
3. **Logs**: Revisa archivos en `/logs`
4. **Comunidad**: Participa en discusiones del repositorio

---

*Ãšltima actualizaciÃ³n: Julio 2025*