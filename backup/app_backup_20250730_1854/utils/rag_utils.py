"""
RAG Utils - Actualizado con ChromaDB + LlamaIndex
Mantiene compatibilidad con el sistema existente
"""
import os
import logging
from typing import List, Dict, Any, Optional
from app.utils.chroma_store import get_chroma_store
from app.services.llamaindex_ingestor import MunicipalDocumentIngestor

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# FUNCIONES PRINCIPALES RAG
# ============================================================================

def buscar_fragmentos_combinados(
    consulta: str, 
    k: int = 5, 
    filtros: Optional[Dict] = None,
    fuente_especifica: Optional[str] = None
) -> List[Dict]:
    """
    B√∫squeda mejorada con ChromaDB y filtros avanzados
    
    Args:
        consulta: Texto de la consulta
        k: N√∫mero de fragmentos a recuperar
        filtros: Filtros de metadatos (ej: {"document_type": "ordenanza"})
        fuente_especifica: Filtrar por fuente espec√≠fica
    
    Returns:
        Lista de fragmentos con metadatos enriquecidos
    """
    try:
        store = get_chroma_store()
        
        # Construir filtros
        search_filters = {}
        if filtros:
            search_filters.update(filtros)
        if fuente_especifica:
            search_filters["fuente"] = fuente_especifica
        
        # Realizar b√∫squeda
        results = store.similarity_search(
            query=consulta,
            k=k,
            filter_metadata=search_filters if search_filters else None
        )
        
        logger.info(f"üîç B√∫squeda '{consulta[:50]}...': {len(results)} fragmentos encontrados")
        
        # Enriquecer resultados con informaci√≥n adicional
        fragmentos_enriquecidos = []
        for i, fragmento in enumerate(results):
            fragmento_enriquecido = {
                **fragmento,
                "ranking": i + 1,
                "relevancia_score": round(1.0 - (i * 0.1), 2),  # Score simulado
                "fragmento_id": fragmento.get("metadata", {}).get("id", f"frag_{i}"),
                "tipo_documento": fragmento.get("metadata", {}).get("document_type", "general")
            }
            fragmentos_enriquecidos.append(fragmento_enriquecido)
        
        return fragmentos_enriquecidos
        
    except Exception as e:
        logger.error(f"‚ùå Error en b√∫squeda combinada: {e}")
        return []

def ingest_documents_with_llamaindex(folder_paths: List[str]) -> int:
    """
    Ingesta documentos usando LlamaIndex + ChromaDB
    
    Args:
        folder_paths: Lista de rutas de carpetas a procesar
    
    Returns:
        N√∫mero total de documentos procesados
    """
    if not folder_paths:
        logger.warning("‚ö†Ô∏è No se proporcionaron carpetas para ingestar")
        return 0
    
    ingestor = MunicipalDocumentIngestor()
    store = get_chroma_store()
    
    total_docs = 0
    
    for folder_path in folder_paths:
        if not os.path.exists(folder_path):
            logger.warning(f"‚ö†Ô∏è Carpeta no encontrada: {folder_path}")
            continue
            
        try:
            logger.info(f"üìÅ Procesando carpeta: {folder_path}")
            
            # Procesar con LlamaIndex
            docs = ingestor.process_municipal_folder(folder_path)
            
            if not docs:
                logger.warning(f"‚ö†Ô∏è No se encontraron documentos en: {folder_path}")
                continue
            
            # Crear chunks especializados
            chunks = ingestor.create_specialized_chunks(docs)
            
            if chunks:
                # Preparar textos y metadatos para ChromaDB
                texts = []
                metadatas = []
                
                for chunk in chunks:
                    texts.append(chunk.text)
                    
                    # Enriquecer metadatos con informaci√≥n de la carpeta
                    metadata = chunk.metadata.copy()
                    metadata.update({
                        "fuente": "documentos",
                        "carpeta_origen": folder_path,
                        "metodo_ingesta": "llamaindex_v2"
                    })
                    metadatas.append(metadata)
                
                # A√±adir a ChromaDB
                ids = store.add_documents(texts, metadatas)
                docs_added = len(ids)
                total_docs += docs_added
                
                logger.info(f"‚úÖ Carpeta {folder_path}: {docs_added} fragmentos ingestados")
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando carpeta {folder_path}: {e}")
            continue
    
    logger.info(f"üéØ Ingesta completada: {total_docs} fragmentos totales")
    return total_docs

def get_vectorstore_stats() -> Dict[str, Any]:
    """Obtener estad√≠sticas del vectorstore actual"""
    try:
        store = get_chroma_store()
        stats = store.get_collection_stats()
        
        # A√±adir informaci√≥n adicional
        stats.update({
            "vectorstore_type": "ChromaDB",
            "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
            "status": "activo"
        })
        
        return stats
        
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo estad√≠sticas: {e}")
        return {"error": str(e), "status": "error"}

# ============================================================================
# FUNCIONES DE COMPATIBILIDAD (mantener interfaz existente)
# ============================================================================

def buscar_fragmentos(consulta: str, k: int = 3) -> List[Dict]:
    """
    Funci√≥n de compatibilidad con c√≥digo existente
    Mapea a la nueva funci√≥n buscar_fragmentos_combinados
    """
    return buscar_fragmentos_combinados(consulta, k)

def buscar_fragmentos_por_fuente(
    consulta: str, 
    fuente: str, 
    k: int = 5
) -> List[Dict]:
    """
    B√∫squeda filtrada por fuente espec√≠fica
    
    Args:
        consulta: Texto de consulta
        fuente: Fuente espec√≠fica (documentos, web, apis, bbdd)
        k: N√∫mero de resultados
    """
    filtros = {"fuente": fuente}
    return buscar_fragmentos_combinados(consulta, k, filtros)

def buscar_por_tipo_documento(
    consulta: str, 
    tipo_documento: str, 
    k: int = 5
) -> List[Dict]:
    """
    B√∫squeda filtrada por tipo de documento municipal
    
    Args:
        consulta: Texto de consulta
        tipo_documento: Tipo (ordenanza, acta, resolucion, etc.)
        k: N√∫mero de resultados
    """
    filtros = {"document_type": tipo_documento}
    return buscar_fragmentos_combinados(consulta, k, filtros)

# ============================================================================
# FUNCIONES DE MIGRACI√ìN Y MANTENIMIENTO
# ============================================================================

def migrate_from_faiss_to_chroma(config_path: str = "app/config/settings.json") -> Dict[str, Any]:
    """
    Migra datos existentes de FAISS a ChromaDB
    
    Args:
        config_path: Ruta al archivo de configuraci√≥n
    
    Returns:
        Reporte de migraci√≥n
    """
    import json
    
    try:
        logger.info("üîÑ Iniciando migraci√≥n FAISS -> ChromaDB...")
        
        # Cargar configuraci√≥n
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # Re-ingestar documentos con nuevo sistema
        folders = config.get("document_folders", [])
        
        if not folders:
            return {"error": "No hay carpetas configuradas", "migrated_docs": 0}
        
        # Limpiar colecci√≥n existente
        store = get_chroma_store()
        try:
            store.delete_collection()
            store = get_chroma_store()  # Recrear
        except:
            pass  # La colecci√≥n puede no existir
        
        # Ingestar con nuevo sistema
        total_docs = ingest_documents_with_llamaindex(folders)
        
        # Generar reporte
        stats = get_vectorstore_stats()
        
        migration_report = {
            "status": "success",
            "migrated_docs": total_docs,
            "vectorstore_stats": stats,
            "migration_timestamp": logger.info.__module__  # Placeholder
        }
        
        logger.info(f"‚úÖ Migraci√≥n completada: {total_docs} documentos")
        return migration_report
        
    except Exception as e:
        logger.error(f"‚ùå Error en migraci√≥n: {e}")
        return {"status": "error", "error": str(e), "migrated_docs": 0}

def cleanup_vectorstore():
    """Limpiar completamente el vectorstore"""
    try:
        store = get_chroma_store()
        store.delete_collection()
        logger.info("üóëÔ∏è Vectorstore limpiado completamente")
        return True
    except Exception as e:
        logger.error(f"‚ùå Error limpiando vectorstore: {e}")
        return False

def reindex_single_folder(folder_path: str) -> int:
    """
    Reindexar una sola carpeta
    
    Args:
        folder_path: Ruta de la carpeta a reindexar
    
    Returns:
        N√∫mero de documentos procesados
    """
    return ingest_documents_with_llamaindex([folder_path])

# ============================================================================
# FUNCIONES DE AN√ÅLISIS Y DEBUGGING
# ============================================================================

def analyze_document_types() -> Dict[str, Any]:
    """Analizar distribuci√≥n de tipos de documentos en el vectorstore"""
    try:
        store = get_chroma_store()
        
        # Buscar todos los documentos por tipo
        types_analysis = {}
        document_types = [
            "ordenanza", "acta", "resolucion", "presupuesto", 
            "convenio", "normativa", "subvencion", "licencia", "documento_general"
        ]
        
        for doc_type in document_types:
            results = store.search_by_metadata(
                {"document_type": doc_type}, 
                limit=1000
            )
            types_analysis[doc_type] = len(results)
        
        # Estad√≠sticas generales
        total_docs = sum(types_analysis.values())
        
        analysis_report = {
            "total_documents": total_docs,
            "document_types": types_analysis,
            "most_common_type": max(types_analysis, key=types_analysis.get) if total_docs > 0 else None,
            "type_diversity": len([t for t, count in types_analysis.items() if count > 0])
        }
        
        return analysis_report
        
    except Exception as e:
        logger.error(f"‚ùå Error en an√°lisis de tipos: {e}")
        return {"error": str(e)}

def test_search_performance(test_queries: List[str], k: int = 5) -> Dict[str, Any]:
    """
    Probar rendimiento de b√∫squeda con consultas de test
    
    Args:
        test_queries: Lista de consultas de prueba
        k: N√∫mero de resultados por consulta
    
    Returns:
        Reporte de rendimiento
    """
    import time
    
    performance_data = {
        "queries_tested": len(test_queries),
        "average_response_time": 0,
        "queries_results": []
    }
    
    total_time = 0
    
    for query in test_queries:
        start_time = time.time()
        
        try:
            results = buscar_fragmentos_combinados(query, k)
            response_time = time.time() - start_time
            
            performance_data["queries_results"].append({
                "query": query,
                "response_time_ms": round(response_time * 1000, 2),
                "results_count": len(results),
                "success": True
            })
            
            total_time += response_time
            
        except Exception as e:
            performance_data["queries_results"].append({
                "query": query,
                "error": str(e),
                "success": False
            })
    
    if len(test_queries) > 0:
        performance_data["average_response_time"] = round(total_time / len(test_queries) * 1000, 2)
    
    return performance_data

# ============================================================================
# CONFIGURACI√ìN Y CONSTANTES
# ============================================================================

# Consultas de test para evaluaci√≥n
DEFAULT_TEST_QUERIES = [
    "¬øQu√© documentos necesito para una licencia de obras?",
    "Ordenanza municipal de ruidos",
    "Presupuesto municipal 2024",
    "Convocatoria de subvenciones",
    "Actas del √∫ltimo pleno",
    "Reglamento de participaci√≥n ciudadana"
]

# Tipos de documentos soportados
SUPPORTED_DOCUMENT_TYPES = [
    "ordenanza", "acta", "resolucion", "presupuesto", 
    "convenio", "normativa", "subvencion", "licencia", "documento_general"
]

# Configuraci√≥n por defecto
DEFAULT_RAG_CONFIG = {
    "chunk_size": 1024,
    "chunk_overlap": 200,
    "max_results": 10,
    "embedding_model": "all-MiniLM-L6-v2"
}